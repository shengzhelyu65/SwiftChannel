models:
  Channelformer:
    batch_size: 16
    num_epochs: 400
    learning_rate: 0.0002
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

  ChannelNet:
    batch_size: 8
    num_epochs: 400
    learning_rate: 0.0003
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

  FSRCNN:
    batch_size: 16
    num_epochs: 400
    learning_rate: 0.0002
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

  I_ResNet:
    batch_size: 16
    num_epochs: 400
    learning_rate: 0.0002
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

  LSiDNN:
    batch_size: 16
    num_epochs: 400
    learning_rate: 0.0002
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

  ReEsNet:
    batch_size: 16
    num_epochs: 400
    learning_rate: 0.0002
    optimizer: 'Adam'
    scheduler: 'LambdaLR'
    lr_step_size: 20
    lr_gamma: 0.5
    train_loss: 'NMSE'
    val_loss: 'NMSE'
    early_stopping_patience: 20
    random_seed: 9048
    num_workers: 8
    device: 'cuda'
    data_augmentation: False

# Default configuration (used if model not found in models section)
default:
  batch_size: 16
  num_epochs: 400
  learning_rate: 0.0002
  optimizer: 'Adam'
  scheduler: 'LambdaLR'
  lr_step_size: 20
  lr_gamma: 0.5
  train_loss: 'NMSE'
  val_loss: 'NMSE'
  early_stopping_patience: 20
  random_seed: 9048
  num_workers: 8
  device: 'cuda'
  data_augmentation: False

